{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuejhovEzuyO4iGaPuv4yn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akankshakusf/Project-DeepLearning-Human-Emotions-Detection-Model/blob/master/DP_Human_Emotion_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4YBavPWxaEz"
      },
      "outputs": [],
      "source": [
        "# **Import Packages and Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import packages\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import pathlib\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from google.colab import files\n",
        "import sklearn\n",
        "import cv2 #computer vision\n",
        "from sklearn.metrics import confusion_matrix, roc_curve\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#tensorflow packages\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import (GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, MaxPool2D, Dense,\n",
        "                                     Flatten, InputLayer, BatchNormalization, Input, Embedding, Permute,\n",
        "                                     Dropout, RandomFlip, RandomRotation, LayerNormalization, MultiHeadAttention,\n",
        "                                     RandomContrast, Rescaling, Resizing, Reshape)\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (Callback, CSVLogger, EarlyStopping, LearningRateScheduler,\n",
        "                                        ModelCheckpoint, ReduceLROnPlateau)\n",
        "from tensorflow.keras.regularizers  import L2, L1"
      ],
      "metadata": {
        "id": "Y_PKZEA3xdy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define main_directory Structure"
      ],
      "metadata": {
        "id": "ISp3aej8TPuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_directory=\"/content/dataset/Emotions Dataset/Emotions Dataset/train\"\n",
        "val_directory = \"/content/dataset/Emotions Dataset/Emotions Dataset/test\""
      ],
      "metadata": {
        "id": "EEAeIaxDxdv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Params Dictionary"
      ],
      "metadata": {
        "id": "QgdNUJCVTUby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "CONFIGURATION={\n",
        "        \"BATCH_SIZE\":32,\n",
        "        \"IM_SIZE\":256,\n",
        "        \"LEARNING_RATE\":0.001,\n",
        "        \"N_EPOCHS\":10,\n",
        "        \"DROPOUT_RATE\":0.0,\n",
        "        \"REGULARIZATION_RATE\":0.0,\n",
        "        \"N_FILTERS\":6,\n",
        "        \"KERNEL_SIZE\":3,\n",
        "        \"N_STRIDES\":1,\n",
        "        \"POOL_SIZE\":2,\n",
        "        \"N_DENSE_1\":100,\n",
        "        \"N_DENSE_2\":10,\n",
        "        \"NUM_CLASSES\":3,\n",
        "        \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "x5UQxWqpxdtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Management"
      ],
      "metadata": {
        "id": "7ijHoRYNTW9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Downloading- Importing Kaggle Dataset"
      ],
      "metadata": {
        "id": "hY5gXptXTXzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "e3J_BUMlTXYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Make a hidden .kaggle folder\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "# Move kaggle.json to the folder\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "\n",
        "# Set permissions\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "gPEx6djuTbzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d muhammadhananasghar/human-emotions-datasethes"
      ],
      "metadata": {
        "id": "uktn7JICTbw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/human-emotions-datasethes.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "id": "_pXCvUNPTbt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Loading"
      ],
      "metadata": {
        "id": "PL85vpVETi6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset= tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_directory,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical', #one hot endoded\n",
        "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
        "    color_mode='rgb',\n",
        "    batch_size=32,\n",
        "    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    shuffle=True,\n",
        "    seed=99,\n",
        ")"
      ],
      "metadata": {
        "id": "i20sNIg7TbqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset= tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_directory,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical', #one hot endoded\n",
        "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
        "    color_mode='rgb',\n",
        "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
        "    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    shuffle=True,\n",
        "    seed=99,\n",
        ")"
      ],
      "metadata": {
        "id": "GrJwEf2RTbnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in val_dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "5thN-CnBTbh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Visualization"
      ],
      "metadata": {
        "id": "PHBTnyzMTp72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch\n",
        "images, labels = next(iter(train_dataset))\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(images[i].numpy() / 255.0)  # normalize and convert tensor to numpy\n",
        "    plt.title(CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(labels[i], axis=0).numpy()])  # convert one-hot to class index\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OSwix-xLTbfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "DLdvC8F8Tti8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- reference: https://www.tensorflow.org/api_docs/python/tf/keras/layers"
      ],
      "metadata": {
        "id": "mUgjU7gfTzEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Augmented layer with \"layers method\" rotation/flip/contrast\n",
        "augment_layers = tf.keras.Sequential([\n",
        "       RandomRotation(factor = (0.25, 0.2501),),\n",
        "       RandomFlip(mode='horizontal',),\n",
        "       RandomContrast(factor=0.1),\n",
        "])"
      ],
      "metadata": {
        "id": "eMpgyJ5GTbbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_layer(image,label):\n",
        "  return augment_layers(image, training=True), label"
      ],
      "metadata": {
        "id": "96l1LP1MTbYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2.Augmented layer with \"layers method\" for resizing and rescaling\n",
        "resize_rescale_layers = tf.keras.Sequential([\n",
        "    Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    Rescaling(1.0 / 255),\n",
        "])"
      ],
      "metadata": {
        "id": "9dUZFuyUTbVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation"
      ],
      "metadata": {
        "id": "K8xLmoaET4ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- let tf pre fetch batches in the background while the model is training on the current batch\n",
        "- let tf automatically choose the optimal number of batches to prefetch based on system performance."
      ],
      "metadata": {
        "id": "Go3kEpaiT6Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prefetch and autotune\n",
        "train_dataset=(train_dataset\n",
        "               .map(augment_layer, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "               .prefetch(tf.data.AUTOTUNE))"
      ],
      "metadata": {
        "id": "Q6GNKRzxTbR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset=(val_dataset\n",
        "             .prefetch(tf.data.AUTOTUNE))"
      ],
      "metadata": {
        "id": "wLt9ZCHuTbPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "_Yr6SSdPTbL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "XdLZry9YTbIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling"
      ],
      "metadata": {
        "id": "AL3MiB5dUBlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Sequential LeNet Model"
      ],
      "metadata": {
        "id": "38m4QQfoUGk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clear up session cache\n",
        "from tensorflow.keras import backend as K\n",
        "# Clear the previous session to reset layer count\n",
        "K.clear_session()\n"
      ],
      "metadata": {
        "id": "3I-sGs-ATbF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. Instantiate the CNN model \"Simple Sequential Model\"\n",
        "'''\n",
        "\n",
        "lenet_model = tf.keras.Sequential([\n",
        "\n",
        "    #InputLayer(shape = (None, None, 3), ),\n",
        "    InputLayer(input_shape = (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"],3)),\n",
        "\n",
        "    resize_rescale_layers, # embedding resize and rescale into SequentialAPI\n",
        "    augment_layers,\n",
        "\n",
        "    # 1st conv layer (extracts basic patterns)\n",
        "    Conv2D(filters=CONFIGURATION[\"N_FILTERS\"], kernel_size=CONFIGURATION[\"KERNEL_SIZE\"],\n",
        "           strides=CONFIGURATION[\"N_STRIDES\"],\n",
        "           padding=\"valid\", activation=\"relu\",\n",
        "           kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size=CONFIGURATION[\"POOL_SIZE\"],\n",
        "              strides=CONFIGURATION[\"N_STRIDES\"]*2),  # Downsamples feature maps\n",
        "\n",
        "    Dropout(rate=CONFIGURATION[\"DROPOUT_RATE\"]),   #add a dropout layer\n",
        "\n",
        "    # 2nd conv layer (extracts deeper features)\n",
        "    Conv2D(filters=CONFIGURATION[\"N_FILTERS\"]*2+4, kernel_size=CONFIGURATION[\"KERNEL_SIZE\"],\n",
        "           strides=CONFIGURATION[\"N_STRIDES\"],\n",
        "           padding=\"valid\", activation=\"relu\",\n",
        "           kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size=CONFIGURATION[\"POOL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"]*2),  # Downsampling again\n",
        "\n",
        "\n",
        "    Flatten(),  # Converts 2D feature maps into 1D array\n",
        "\n",
        "    Dense(CONFIGURATION[\"N_DENSE_1\"], activation=\"relu\",kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),  # Fully connected layer\n",
        "    BatchNormalization(),\n",
        "    Dropout(rate=CONFIGURATION[\"DROPOUT_RATE\"]),   #add a dropout layer\n",
        "\n",
        "    Dense(CONFIGURATION[\"N_DENSE_2\"], activation=\"relu\",\n",
        "          kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),   # Further processing\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(CONFIGURATION[\"NUM_CLASSES\"], activation=\"softmax\"),     # Output layer (multi-class classification)\n",
        "\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "lenet_model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "K8s5rqXOTbCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n"
      ],
      "metadata": {
        "id": "BK3R4XbhUNKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = CategoricalCrossentropy()\n",
        "#loss_function = SparseCategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "ldDwm-NZTa_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [CategoricalAccuracy(name = \"accuracy\"), TopKCategoricalAccuracy(k=2, name = \"top_k_accuracy\")]"
      ],
      "metadata": {
        "id": "I70qKTNYTa8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "lenet_model.compile(\n",
        "    optimizer=Adam(learning_rate=CONFIGURATION[\"LEARNING_RATE\"]),\n",
        "    loss=loss_function,\n",
        "    metrics=metrics)"
      ],
      "metadata": {
        "id": "o6yTaA7sTa5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model\n",
        "history = lenet_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=CONFIGURATION[\"N_EPOCHS\"],\n",
        "    verbose=True)"
      ],
      "metadata": {
        "id": "hWch12auTa2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "2VPMcrglUVN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AN_8fI_xTazz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Losses')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train_loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "obQuWp42Tawu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "GAoFiqlRUZpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate model on val data\n",
        "lenet_model.evaluate(val_dataset)"
      ],
      "metadata": {
        "id": "bfdv5fluTatl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing"
      ],
      "metadata": {
        "id": "3ed10lCCUbZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1. testing for happy image"
      ],
      "metadata": {
        "id": "1szA4XiQUgQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test an image\n",
        "test_image = cv2.imread(\"/content/dataset/Emotions Dataset/Emotions Dataset/test/happy/111073.jpg\")\n",
        "\n",
        "# Convert the image to tensors\n",
        "im = tf.constant(test_image, dtype=tf.float32)\n",
        "print(im.shape) # but this needs to be 4 dimensional\n",
        "\n",
        "\n",
        "#adding batch dimension to shape [1, height, width, channels])\n",
        "im = tf.expand_dims(im, axis=0)\n",
        "print(im.shape)\n",
        "\n",
        "#get predictions (argmax: get max probabilities)\n",
        "predictions= lenet_model(im)\n",
        "print(CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(predictions, axis=-1).numpy()[0]])"
      ],
      "metadata": {
        "id": "VFjOPSH5Ucv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2. testing for sad image"
      ],
      "metadata": {
        "id": "-VYul9zCUjL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test an image\n",
        "test_image = cv2.imread(\"/content/dataset/Emotions Dataset/Emotions Dataset/test/sad/105565.jpg_brightness_1.jpg\")\n",
        "\n",
        "# Convert jpg to tensors\n",
        "im = tf.constant(test_image, dtype=tf.float32)\n",
        "print(im.shape) # but this needs to be 4 dimensional\n",
        "\n",
        "#adding batch dimension to shape [1, height, width, channels])\n",
        "im = tf.expand_dims(im, axis=0)\n",
        "print(im.shape)\n",
        "\n",
        "#get predictions (argmax: get max probabilities)\n",
        "predictions= lenet_model(im)\n",
        "print(CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(predictions, axis=-1).numpy()[0]])"
      ],
      "metadata": {
        "id": "smPFyaCsUcst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch\n",
        "images, labels = next(iter(train_dataset))\n",
        "plt.figure(figsize=(9, 9))\n",
        "\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(images[i].numpy()/ 255.0)  # normalize and convert tensor to numpy\n",
        "\n",
        "    #titles on images\n",
        "    plt.title(\"True Labels: \" + CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(labels[i], axis=0).numpy()]+ \"\\n\" +\n",
        "              \"Predicted Labels: \" + CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(lenet_model(tf.expand_dims(images[i], axis=0)), axis=-1).numpy()[0]])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T-6PCz1WUcp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "IDy9tMvLUodM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted= []\n",
        "labels= []\n",
        "\n",
        "#iterate through the validation ds\n",
        "for im, label in val_dataset:\n",
        "  predicted.append(lenet_model(im))\n",
        "  labels.append(label.numpy())"
      ],
      "metadata": {
        "id": "XQBqZ9FhUcm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#leave the last batch of 32 and take rest & later flatten the batches\n",
        "\n",
        "#leaving last batch for now [:-1]\n",
        "print(np.argmax(labels[:-1], axis= -1).flatten())\n",
        "\n",
        "#taking just last batch [-1]\n",
        "print(np.argmax(labels[-1], axis= -1).flatten())\n",
        "\n",
        "#for labels combining last batch and all batch through concatenation\n",
        "print(np.concatenate([np.argmax(labels[:-1], axis= -1).flatten(),np.argmax(labels[-1], axis= -1).flatten()]))\n",
        "\n",
        "#for predicted values combining last batch and all batch through concatenation\n",
        "print(np.concatenate([np.argmax(predicted[:-1], axis= -1).flatten(),np.argmax(predicted[-1], axis= -1).flatten()]))"
      ],
      "metadata": {
        "id": "D50ZXde3UrjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicted and true values\n",
        "predicted = np.concatenate([np.argmax(predicted[:-1], axis= -1).flatten(),np.argmax(predicted[-1], axis= -1).flatten()]) #what model predicted\n",
        "labels = np.concatenate([np.argmax(labels[:-1], axis= -1).flatten(),np.argmax(labels[-1], axis= -1).flatten()]) #true values\n"
      ],
      "metadata": {
        "id": "QjKD8yUCUsob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "cm=confusion_matrix(predicted,labels)\n",
        "print(cm)\n",
        "\n",
        "sns.heatmap(cm,annot=True, fmt='g',cmap=\"crest\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")"
      ],
      "metadata": {
        "id": "_eWHvAFAUsmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-The model performed best in recognizing happy, correctly classifying 879 samples. However, it misclassified 132 as angry and 214 as sad.\n",
        "\n",
        "-For the sad class, the model correctly identified 475 cases, while incorrectly classifying 87 as angry and 93 as happy.\n",
        "\n",
        "-Recognition of the angry class was weakest, correctly classifying only 294 cases, with 31 misclassified as happy and 67 as sad.\n",
        "\n",
        "-The large off-diagonal values in the \"happy\" column indicate a bias towards predicting \"happy\" more frequently, even when the actual emotion is \"angry\" or \"sad\"."
      ],
      "metadata": {
        "id": "XTLlHlp_Uwg5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_7wOgv4bUsjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BPJyFjgrUshC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FGFBbO9PUsee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-kXEOGoUsbu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}